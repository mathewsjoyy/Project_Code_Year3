{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import requests\n",
    "import random\n",
    "\n",
    "# List of URLs to scrape with the year as the key and the URL as the value\n",
    "pages_to_scrape = {\"2022-2023\": \"https://sofifa.com/teams?type=all&lg%5B0%5D=13&r=230054&set=true&showCol%5B0%5D=oa&showCol%5B1%5D=sa&showCol%5B2%5D=at&showCol%5B3%5D=md&showCol%5B4%5D=df&col=oa&sort=desc&hl=en-US\",\n",
    "                   \"2021-2022\": \"https://sofifa.com/teams?type=all&lg%5B0%5D=13&r=220069&set=true&showCol%5B0%5D=oa&showCol%5B1%5D=sa&showCol%5B2%5D=at&showCol%5B3%5D=md&showCol%5B4%5D=df&col=oa&sort=desc&hl=en-US\",\n",
    "                   \"2020-2021\": \"https://sofifa.com/teams?type=all&lg%5B0%5D=13&r=210064&set=true&showCol%5B0%5D=oa&showCol%5B1%5D=sa&showCol%5B2%5D=at&showCol%5B3%5D=md&showCol%5B4%5D=df&col=oa&sort=desc&hl=en-US\",\n",
    "                   \"2019-2020\": \"https://sofifa.com/teams?type=all&lg%5B0%5D=13&r=200061&set=true&showCol%5B0%5D=oa&showCol%5B1%5D=sa&showCol%5B2%5D=at&showCol%5B3%5D=md&showCol%5B4%5D=df&col=oa&sort=desc&hl=en-US\",\n",
    "                   \"2018-2019\": \"https://sofifa.com/teams?type=all&lg%5B0%5D=13&r=190075&set=true&showCol%5B0%5D=oa&showCol%5B1%5D=sa&showCol%5B2%5D=at&showCol%5B3%5D=md&showCol%5B4%5D=df&col=oa&sort=desc&hl=en-US\",\n",
    "                   \"2017-2018\": \"https://sofifa.com/teams?type=all&lg%5B0%5D=13&r=180084&set=true&showCol%5B0%5D=oa&showCol%5B1%5D=sa&showCol%5B2%5D=at&showCol%5B3%5D=md&showCol%5B4%5D=df&col=oa&sort=desc&hl=en-US\"}\n",
    "\n",
    "# Define a function to check the status code of the page\n",
    "def check_status_code(page_to_scrape):\n",
    "    if page_to_scrape.status_code == 200:\n",
    "        print('Page was successfully scraped!\\n')\n",
    "    elif page_to_scrape.status_code == 404:\n",
    "        print('Page was not found!\\n')\n",
    "    elif page_to_scrape.status_code == 500:\n",
    "        print('Internal server error!\\n')\n",
    "    elif page_to_scrape.status_code == 403:\n",
    "        print('Access denied!\\n')\n",
    "    else:\n",
    "        print('Unknown error!\\n')\n",
    "\n",
    "# Define a list of user agents we can rotate through to avoid getting blocked \n",
    "user_agent_list = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18363',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page was successfully scraped!\n",
      "\n",
      "Season: 2022-2023\n",
      "Manchester City, 85\n",
      "Liverpool, 84\n",
      "Arsenal, 82\n",
      "Chelsea, 82\n",
      "Manchester United, 82\n",
      "Tottenham Hotspur, 81\n",
      "Newcastle United, 80\n",
      "Aston Villa, 79\n",
      "West Ham United, 78\n",
      "Leicester City, 78\n",
      "Wolverhampton Wanderers, 78\n",
      "Nottingham Forest, 77\n",
      "Brighton & Hove Albion, 77\n",
      "Everton, 76\n",
      "Crystal Palace, 76\n",
      "Fulham, 76\n",
      "Leeds United, 75\n",
      "Southampton, 75\n",
      "Brentford, 75\n",
      "AFC Bournemouth, 74\n",
      "Page was successfully scraped!\n",
      "\n",
      "Season: 2021-2022\n",
      "Liverpool, 85\n",
      "Manchester City, 85\n",
      "Chelsea, 84\n",
      "Manchester United, 83\n",
      "Tottenham Hotspur, 81\n",
      "Arsenal, 80\n",
      "Leicester City, 80\n",
      "West Ham United, 79\n",
      "Aston Villa, 78\n",
      "Everton, 78\n",
      "Wolverhampton Wanderers, 78\n",
      "Newcastle United, 77\n",
      "Burnley, 76\n",
      "Crystal Palace, 76\n",
      "Leeds United, 76\n",
      "Brighton & Hove Albion, 76\n",
      "Southampton, 76\n",
      "Watford, 74\n",
      "Brentford, 74\n",
      "Norwich City, 73\n",
      "Page was successfully scraped!\n",
      "\n",
      "Season: 2020-2021\n",
      "Liverpool, 85\n",
      "Manchester City, 85\n",
      "Chelsea, 82\n",
      "Manchester United, 82\n",
      "Tottenham Hotspur, 82\n",
      "Arsenal, 80\n",
      "Leicester City, 80\n",
      "Everton, 79\n",
      "Wolverhampton Wanderers, 79\n",
      "West Ham United, 78\n",
      "Aston Villa, 77\n",
      "Burnley, 76\n",
      "Crystal Palace, 76\n",
      "Leeds United, 76\n",
      "Newcastle United, 76\n",
      "Southampton, 76\n",
      "Brighton & Hove Albion, 75\n",
      "Fulham, 75\n",
      "Sheffield United, 73\n",
      "West Bromwich Albion, 73\n",
      "Page was successfully scraped!\n",
      "\n",
      "Season: 2019-2020\n",
      "Liverpool, 85\n",
      "Manchester City, 85\n",
      "Tottenham Hotspur, 82\n",
      "Chelsea, 81\n",
      "Manchester United, 81\n",
      "Arsenal, 80\n",
      "Leicester City, 79\n",
      "Everton, 78\n",
      "West Ham United, 78\n",
      "Wolverhampton Wanderers, 78\n",
      "Watford, 77\n",
      "Crystal Palace, 77\n",
      "Newcastle United, 77\n",
      "Aston Villa, 76\n",
      "Burnley, 76\n",
      "Brighton & Hove Albion, 76\n",
      "Southampton, 76\n",
      "AFC Bournemouth, 76\n",
      "Sheffield United, 75\n",
      "Norwich City, 74\n",
      "Page was successfully scraped!\n",
      "\n",
      "Season: 2018-2019\n",
      "Manchester City, 85\n",
      "Chelsea, 83\n",
      "Liverpool, 83\n",
      "Tottenham Hotspur, 83\n",
      "Arsenal, 82\n",
      "Manchester United, 82\n",
      "Everton, 79\n",
      "West Ham United, 78\n",
      "Leicester City, 78\n",
      "Watford, 77\n",
      "Burnley, 77\n",
      "Crystal Palace, 77\n",
      "Wolverhampton Wanderers, 77\n",
      "AFC Bournemouth, 77\n",
      "Newcastle United, 76\n",
      "Brighton & Hove Albion, 76\n",
      "Southampton, 76\n",
      "Fulham, 75\n",
      "Huddersfield Town, 74\n",
      "Cardiff City, 73\n",
      "Page was successfully scraped!\n",
      "\n",
      "Season: 2017-2018\n",
      "Manchester City, 84\n",
      "Chelsea, 83\n",
      "Manchester United, 83\n",
      "Tottenham Hotspur, 83\n",
      "Arsenal, 82\n",
      "Liverpool, 81\n",
      "Everton, 79\n",
      "Leicester City, 78\n",
      "Watford, 77\n",
      "Burnley, 77\n",
      "West Ham United, 77\n",
      "Crystal Palace, 76\n",
      "Stoke City, 76\n",
      "Southampton, 76\n",
      "West Bromwich Albion, 76\n",
      "AFC Bournemouth, 76\n",
      "Newcastle United, 75\n",
      "Brighton & Hove Albion, 75\n",
      "Swansea City, 75\n",
      "Huddersfield Town, 74\n"
     ]
    }
   ],
   "source": [
    "# Global list to store the data\n",
    "seasonal_team_rating = []\n",
    "\n",
    "# Loop through the pages to scrape\n",
    "for season, page in pages_to_scrape.items():\n",
    "    # Pick a random user agent\n",
    "    headers = {\"User-Agent\": user_agent_list[random.randint(0, len(user_agent_list)-1)]}\n",
    "\n",
    "    # Get the page to scrape\n",
    "    page_to_scrape = requests.get(page, headers=headers)\n",
    "\n",
    "    # Check if the page was successfully scraped\n",
    "    check_status_code(page_to_scrape)\n",
    "\n",
    "    # Create a BeautifulSoup object and parse the HTML\n",
    "    soup = BeautifulSoup(page_to_scrape.text, 'lxml')\n",
    "\n",
    "    # Convert the BeautifulSoup object into an lxml object\n",
    "    tree = html.fromstring(str(soup))\n",
    "\n",
    "    # Use the full XPath to find the data cards we want\n",
    "    data = tree.xpath('/html/body/div[1]/div/div[2]/div/table/tbody')\n",
    "\n",
    "    # Get the list of individual data cards\n",
    "    cards = data[0].xpath('tr')\n",
    "    \n",
    "    # Define variables to store the data\n",
    "    team_name = \"\"\n",
    "    team_overall = \"\"\n",
    "\n",
    "    # Loop through each data card and extract the data\n",
    "    print(f\"Season: {season}\")\n",
    "    for card in cards:\n",
    "        # Get the team name\n",
    "        team_name = card[1][0].text_content()\n",
    "        \n",
    "        # Get the team overall rating\n",
    "        team_overall = card[2].text_content()\n",
    "        \n",
    "        print(f\"{team_name}, {team_overall}\")\n",
    "    \n",
    "        # Append the data to the global list\n",
    "        seasonal_team_rating.append({\"Season\": season, \"Team\": team_name, \"Rating\": team_overall})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert our list of dictionaries into a Pandas DataFrame\n",
    "df = pd.DataFrame(seasonal_team_rating)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"Dataset/seasonal_team_rating.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
